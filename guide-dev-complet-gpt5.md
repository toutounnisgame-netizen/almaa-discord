# ğŸ“– GUIDE DÃ‰VELOPPEMENT COMPLET ALMAA WORKSPACE - POUR GPT-5

## ğŸ¯ CONTEXTE ET OBJECTIFS

### Situation Actuelle
Tu dÃ©veloppes avec moi la **Phase 4** du projet ALMAA Workspace, une Ã©volution majeure d'un prototype Discord IA existant vers un assistant AGI personnel complet.

**Objectif Principal** : CrÃ©er un systÃ¨me multi-agents spÃ©cialisÃ©s avec :
- SystÃ¨me de pertinence contextuelle (agents parlent quand c'est pertinent)
- Interface admin complÃ¨te avec contrÃ´le des agents
- Outils modulaires pour productivitÃ©
- Gouvernance IA avec modÃ©ration autonome
- Architecture scalable pour 100+ agents

### Utilisateur Final
- **Profil** : Chef de projet en arrÃªt maladie, budget 200kâ‚¬, serveur 9600 TOPS prÃ©vu
- **Objectif** : Assistant AGI personnel pour tous projets (dÃ©veloppement, Ã©criture, crÃ©ativitÃ©, analyse)
- **Contraintes** : 100% offline, sÃ©curitÃ© maximale, temps illimitÃ© de dÃ©veloppement
- **Use Case #1** : Agents l'aident Ã  dÃ©velopper le projet ALMAA lui-mÃªme

---

## ğŸ“‹ INSTRUCTIONS SPÃ‰CIFIQUES DÃ‰VELOPPEMENT

### âš ï¸ RÃˆGLES CRITIQUES Ã€ RESPECTER

1. **Base Existante** : Tu pars du code existant GitHub (architecture dÃ©jÃ  en place)
2. **Approche IncrÃ©mentale** : AmÃ©liore le code existant, pas de reconstruction complÃ¨te
3. **Analyse Avant Action** : Toujours analyser le code existant avant de proposer modifications
4. **Documentation ComplÃ¨te** : Documente chaque modification avec commentaires dÃ©taillÃ©s
5. **Tests Inclus** : Inclus tests pour chaque nouvelle fonctionnalitÃ©
6. **Configuration Flexible** : Rends tout configurable via fichiers YAML/JSON
7. **Performance** : Optimise pour temps rÃ©el et charge Ã©levÃ©e
8. **SÃ©curitÃ©** : Valide toutes les entrÃ©es, sandboxe les exÃ©cutions

### ğŸ› ï¸ STACK TECHNIQUE EXISTANTE
```yaml
Backend:
  api: "FastAPI + Uvicorn"
  database: "PostgreSQL 16 + pgvector"
  cache: "Redis 7 Alpine"
  vector_db: "ChromaDB"
  storage: "MinIO S3-compatible"
  ai: "Ollama + modÃ¨les locaux"
  
Frontend:
  framework: "Next.js (Static Site Generation)"
  ui: "React + TypeScript"
  styling: "Tailwind CSS"
  state: "React hooks + Context"
  
Infrastructure:
  proxy: "Nginx Alpine"
  containers: "Docker Compose"
  monitoring: "Prometheus + Grafana"
  logging: "Loki + Grafana"
```

---

## ğŸ¯ ROADMAP DÃ‰VELOPPEMENT PRIORITAIRE

### PHASE 1 : FOUNDATION (Semaines 1-4) - PRIORITÃ‰ CRITIQUE

#### Semaine 1-2 : SystÃ¨me de Pertinence Contextuelle
**Objectif** : Agents parlent seulement quand c'est pertinent (score > 0.5)

**DÃ©veloppements requis :**

1. **Service Relevance Engine**
```python
# Ã€ crÃ©er : relevance-engine/
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ relevance_calculator.py    # Calcul scores pertinence
â”‚   â”œâ”€â”€ context_analyzer.py        # Analyse contexte conversation
â”‚   â”œâ”€â”€ decision_engine.py         # DÃ©cision intervention agent
â”‚   â””â”€â”€ learning_system.py         # Apprentissage patterns
â”œâ”€â”€ models/
â”‚   â””â”€â”€ relevance_factors.py       # Facteurs pertinence
â”œâ”€â”€ api/
â”‚   â””â”€â”€ relevance_api.py           # API REST endpoints
â””â”€â”€ config/
    â””â”€â”€ relevance_config.yaml      # Configuration facteurs
```

**Facteurs de pertinence Ã  implÃ©menter :**
- `expertise_match` : Correspondance expertise agent / sujet conversation
- `conversation_gap` : DÃ©tection manque d'information dans conversation
- `workload_capacity` : Charge de travail actuelle agent
- `recent_participation` : Ã‰viter participation rÃ©pÃ©tÃ©e
- `relationship_trust` : Niveau confiance avec autres agents
- `timing_appropriateness` : Moment appropriÃ© pour intervenir

2. **IntÃ©gration WebSocket pour Temps RÃ©el**
```python
# Extension WebSocket existant
websocket_manager.py:
  - add_relevance_checking()
  - integrate_decision_engine()
  - real_time_score_updates()
```

3. **Extension API FastAPI**
```python
# Nouveaux endpoints Ã  ajouter
/api/v1/relevance/
â”œâ”€â”€ POST /calculate          # Calcul score pertinence
â”œâ”€â”€ GET  /scores/{agent_id}  # Scores agent
â”œâ”€â”€ PUT  /thresholds         # Mise Ã  jour seuils
â””â”€â”€ GET  /analytics          # Analytics pertinence
```

#### Semaine 3-4 : Interface Admin Basique
**Objectif** : Dashboard avec contrÃ´les agents essentiels

**DÃ©veloppements requis :**

1. **Dashboard Admin (Next.js)**
```typescript
// Composants Ã  crÃ©er
components/admin/
â”œâ”€â”€ AgentControlPanel.tsx      # Panneau contrÃ´le principal
â”œâ”€â”€ AgentCard.tsx              # Carte agent individuel
â”œâ”€â”€ PauseResumeButton.tsx      # ContrÃ´les pause/reprendre
â”œâ”€â”€ TaskInjector.tsx           # Interface injection tÃ¢ches
â”œâ”€â”€ DebugMonitor.tsx           # Monitoring debug temps rÃ©el
â”œâ”€â”€ NotificationCenter.tsx     # Centre notifications
â””â”€â”€ SystemMetrics.tsx          # MÃ©triques systÃ¨me

pages/admin/
â”œâ”€â”€ dashboard.tsx              # Dashboard principal
â”œâ”€â”€ agents.tsx                 # Gestion agents
â””â”€â”€ debug.tsx                  # Interface debug
```

2. **API Extensions**
```python
# Nouveaux endpoints admin
/api/v1/admin/
â”œâ”€â”€ POST /agents/{id}/pause    # Pause agent
â”œâ”€â”€ POST /agents/{id}/resume   # Reprendre agent
â”œâ”€â”€ POST /tasks/inject         # Injection tÃ¢che prioritaire
â”œâ”€â”€ GET  /debug/{agent_id}     # Debug infos agent
â””â”€â”€ GET  /notifications        # Notifications admin
```

### PHASE 2 : AGENT MANAGEMENT (Semaines 5-8)

#### Templates d'Agents PrÃ©-configurÃ©s
**Objectif** : Templates spÃ©cialisÃ©s (Developer, Analyst, Moderator, etc.)

**Templates essentiels :**
```yaml
Developer Backend:
  model: "codellama:7b"
  context_size: 8192
  personality: "Rigoureux, mÃ©thodique, orientÃ© solution"
  tools: ["git", "code-analysis", "testing", "docker"]
  system_prompt: "Tu es un expert dÃ©veloppeur backend Python/FastAPI..."
  
Analyst Data:
  model: "llama3.1:8b"
  context_size: 16384
  personality: "Analytique, prÃ©cis, synthÃ©tique"
  tools: ["data-processing", "visualization", "research"]
  system_prompt: "Tu es un expert en analyse de donnÃ©es..."
  
Moderator Quality:
  model: "mistral:7b"
  context_size: 8192
  personality: "Ã‰quilibrÃ©, juste, diplomatique"
  tools: ["moderation", "quality-check", "conflict-resolution"]
  system_prompt: "Tu es un superviseur qualitÃ© et mÃ©diateur..."
```

#### Communication Inter-Agents
**Objectif** : Messages privÃ©s entre agents + canaux deepthink

**DÃ©veloppements requis :**
1. **Messages PrivÃ©s** : SystÃ¨me MP avec acceptation/refus
2. **Canaux Deepthink** : Canaux privÃ©s rÃ©flexion individuelle
3. **Archivage Automatique** : Archivage conversations aprÃ¨s 48h inactivitÃ©

---

## ğŸ’» STANDARDS DÃ‰VELOPPEMENT

### Structure Code
```python
# Standard structure pour nouveaux services
service-name/
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ main.py                    # Point d'entrÃ©e service
â”‚   â”œâ”€â”€ core/                      # Logique mÃ©tier core
â”‚   â”œâ”€â”€ api/                       # Endpoints API REST
â”‚   â”œâ”€â”€ models/                    # ModÃ¨les donnÃ©es
â”‚   â”œâ”€â”€ services/                  # Services mÃ©tier
â”‚   â”œâ”€â”€ utils/                     # Utilitaires
â”‚   â””â”€â”€ exceptions/                # Exceptions personnalisÃ©es
â”œâ”€â”€ tests/
â”‚   â”œâ”€â”€ unit/                      # Tests unitaires
â”‚   â”œâ”€â”€ integration/               # Tests intÃ©gration
â”‚   â””â”€â”€ fixtures/                  # Fixtures tests
â”œâ”€â”€ config/
â”‚   â”œâ”€â”€ settings.yaml              # Configuration service
â”‚   â””â”€â”€ logging.yaml               # Configuration logs
â”œâ”€â”€ docker/
â”‚   â”œâ”€â”€ Dockerfile                 # Image Docker
â”‚   â””â”€â”€ docker-compose.test.yml    # Tests Docker
â”œâ”€â”€ requirements/
â”‚   â”œâ”€â”€ base.txt                   # DÃ©pendances base
â”‚   â””â”€â”€ dev.txt                    # DÃ©pendances dÃ©veloppement
â””â”€â”€ README.md                      # Documentation service
```

### Standards Codage
```python
# Exemple standard fonction
async def calculate_relevance_score(
    agent_id: str,
    conversation_context: ConversationContext,
    current_message: Message
) -> RelevanceScore:
    """
    Calcule le score de pertinence pour qu'un agent intervienne.
    
    Args:
        agent_id: ID unique de l'agent
        conversation_context: Contexte conversation actuelle
        current_message: Message dÃ©clencheur
        
    Returns:
        RelevanceScore: Score et dÃ©tails facteurs
        
    Raises:
        AgentNotFoundError: Si agent n'existe pas
        ContextInvalidError: Si contexte invalide
    """
    try:
        # Validation inputs
        await validate_agent_exists(agent_id)
        validate_conversation_context(conversation_context)
        
        # RÃ©cupÃ©ration Ã©tat agent
        agent_state = await get_agent_state(agent_id)
        
        # Calcul facteurs pertinence
        factors = await calculate_factors(
            agent_state=agent_state,
            context=conversation_context,
            message=current_message
        )
        
        # Score final pondÃ©rÃ©
        final_score = weighted_sum(factors, agent_state.personality.weights)
        
        # Log pour debug
        logger.debug(
            f"Relevance calculated for agent {agent_id}: {final_score}",
            extra={"agent_id": agent_id, "score": final_score, "factors": factors}
        )
        
        return RelevanceScore(
            agent_id=agent_id,
            score=final_score,
            factors=factors,
            timestamp=datetime.utcnow()
        )
        
    except Exception as e:
        logger.error(f"Error calculating relevance: {e}", exc_info=True)
        raise RelevanceCalculationError(f"Failed to calculate relevance: {e}")
```

### Standards Tests
```python
# Exemple test complet
import pytest
from unittest.mock import AsyncMock, patch
from fastapi.testclient import TestClient

class TestRelevanceCalculator:
    
    @pytest.fixture
    async def sample_agent(self):
        return Agent(
            id="agent-123",
            name="Test Developer",
            type="developer",
            status="active",
            expertise=["python", "fastapi"]
        )
    
    @pytest.fixture
    def sample_conversation(self):
        return ConversationContext(
            channel_id="channel-456",
            recent_messages=[
                Message(content="How to optimize FastAPI performance?"),
                Message(content="We need better database queries")
            ],
            participants=["agent-789", "user-admin"],
            topic="performance optimization"
        )
    
    async def test_calculate_relevance_score_high_match(
        self, sample_agent, sample_conversation
    ):
        """Test score Ã©levÃ© quand expertise correspond parfaitement."""
        
        with patch('relevance_calculator.get_agent_state') as mock_get_state:
            mock_get_state.return_value = sample_agent
            
            result = await calculate_relevance_score(
                agent_id=sample_agent.id,
                conversation_context=sample_conversation,
                current_message=Message(content="FastAPI optimization tips?")
            )
            
            assert result.score > 0.8
            assert "expertise_match" in result.factors
            assert result.factors["expertise_match"] > 0.9
    
    async def test_calculate_relevance_score_low_match(
        self, sample_agent, sample_conversation
    ):
        """Test score faible quand pas d'expertise correspondante."""
        
        off_topic_conversation = ConversationContext(
            channel_id="channel-789",
            recent_messages=[Message(content="Best cooking recipes?")],
            topic="cooking"
        )
        
        with patch('relevance_calculator.get_agent_state') as mock_get_state:
            mock_get_state.return_value = sample_agent
            
            result = await calculate_relevance_score(
                agent_id=sample_agent.id,
                conversation_context=off_topic_conversation,
                current_message=Message(content="Recipe for pasta?")
            )
            
            assert result.score < 0.3
            assert result.factors["expertise_match"] < 0.2
    
    async def test_calculate_relevance_agent_not_found(self):
        """Test erreur quand agent n'existe pas."""
        
        with pytest.raises(AgentNotFoundError):
            await calculate_relevance_score(
                agent_id="non-existent",
                conversation_context=ConversationContext(),
                current_message=Message(content="test")
            )
```

### Standards Configuration
```yaml
# Exemple configuration service
# config/relevance_config.yaml
relevance_engine:
  thresholds:
    intervention: 0.5      # Score minimum pour intervention
    high_priority: 0.8     # Score pour intervention prioritaire
    low_priority: 0.2      # Score pour Ã©coute passive uniquement
    
  factors:
    expertise_match:
      weight: 0.3
      enabled: true
      min_threshold: 0.1
      
    conversation_gap:
      weight: 0.25
      enabled: true
      detection_method: "semantic_similarity"
      
    workload_capacity:
      weight: 0.2
      enabled: true
      max_concurrent_tasks: 3
      
    recent_participation:
      weight: 0.15
      enabled: true
      cooldown_minutes: 30
      
    timing_appropriateness:
      weight: 0.1
      enabled: true
      avoid_interruption: true
      
  performance:
    cache_ttl: 300        # TTL cache scores (5 minutes)
    calculation_timeout: 5 # Timeout calcul (5 secondes)
    batch_size: 10        # Taille lot pour traitement
    
  logging:
    level: "INFO"
    include_factors: true
    include_scores: true
```

---

## ğŸ—ƒï¸ BASES DE DONNÃ‰ES ET MODÃˆLES

### ModÃ¨les PostgreSQL Existants Ã  Ã‰tendre
```sql
-- Extensions nÃ©cessaires pour nouveaux modÃ¨les
CREATE EXTENSION IF NOT EXISTS "uuid-ossp";
CREATE EXTENSION IF NOT EXISTS "pgvector";

-- Nouveau schÃ©ma pour pertinence
CREATE SCHEMA IF NOT EXISTS relevance;

-- Table scores de pertinence
CREATE TABLE relevance.scores (
    id UUID PRIMARY KEY DEFAULT uuid_generate_v4(),
    agent_id UUID NOT NULL REFERENCES agents(id),
    conversation_context JSONB NOT NULL,
    score REAL NOT NULL CHECK (score >= 0 AND score <= 1),
    factors JSONB NOT NULL,
    intervention_decision BOOLEAN NOT NULL,
    created_at TIMESTAMP WITH TIME ZONE DEFAULT NOW(),
    
    -- Index pour performance
    INDEX idx_relevance_agent_created (agent_id, created_at DESC),
    INDEX idx_relevance_score_created (score DESC, created_at DESC)
);

-- Table historique dÃ©cisions
CREATE TABLE relevance.decisions (
    id UUID PRIMARY KEY DEFAULT uuid_generate_v4(),
    agent_id UUID NOT NULL REFERENCES agents(id),
    channel_id UUID NOT NULL REFERENCES channels(id),
    message_id UUID REFERENCES messages(id),
    decision_type VARCHAR(50) NOT NULL, -- 'intervene', 'observe', 'reflect'
    relevance_score REAL NOT NULL,
    factors JSONB NOT NULL,
    outcome VARCHAR(50), -- 'success', 'ignored', 'conflict'
    feedback_score REAL, -- Feedback qualitÃ© intervention
    created_at TIMESTAMP WITH TIME ZONE DEFAULT NOW()
);
```

### ModÃ¨les Pydantic
```python
# models/relevance_models.py
from pydantic import BaseModel, Field, validator
from typing import Dict, List, Optional, Any
from datetime import datetime
from enum import Enum

class RelevanceFactor(BaseModel):
    """Facteur individuel de pertinence."""
    name: str = Field(..., description="Nom du facteur")
    value: float = Field(..., ge=0, le=1, description="Valeur du facteur [0-1]")
    weight: float = Field(..., ge=0, le=1, description="Poids du facteur [0-1]")
    explanation: Optional[str] = Field(None, description="Explication facteur")
    metadata: Dict[str, Any] = Field(default_factory=dict)

class ConversationContext(BaseModel):
    """Contexte de conversation pour calcul pertinence."""
    channel_id: str = Field(..., description="ID du canal")
    server_id: str = Field(..., description="ID du serveur")
    recent_messages: List[Dict[str, Any]] = Field(default_factory=list)
    participants: List[str] = Field(default_factory=list)
    topic: Optional[str] = Field(None, description="Sujet conversation")
    urgency_level: float = Field(0.5, ge=0, le=1, description="Niveau urgence")
    conversation_flow: str = Field("normal", description="Type de flow")

class AgentState(BaseModel):
    """Ã‰tat actuel d'un agent."""
    agent_id: str
    status: str = Field(..., regex="^(active|paused|working|error)$")
    current_workload: float = Field(0, ge=0, le=1, description="Charge actuelle")
    expertise_areas: List[str] = Field(default_factory=list)
    performance_score: float = Field(0.5, ge=0, le=1)
    recent_activity: Dict[str, Any] = Field(default_factory=dict)
    personality_weights: Dict[str, float] = Field(default_factory=dict)
    last_intervention: Optional[datetime] = None

class RelevanceScore(BaseModel):
    """Score de pertinence calculÃ©."""
    agent_id: str
    score: float = Field(..., ge=0, le=1, description="Score final [0-1]")
    factors: Dict[str, RelevanceFactor] = Field(..., description="DÃ©tail facteurs")
    decision: str = Field(..., regex="^(intervene|observe|reflect)$")
    confidence: float = Field(..., ge=0, le=1, description="Confiance dÃ©cision")
    explanation: str = Field(..., description="Explication dÃ©cision")
    timestamp: datetime = Field(default_factory=datetime.utcnow)
    
    @validator('factors')
    def validate_factors_sum_weights(cls, v):
        total_weight = sum(factor.weight for factor in v.values())
        if not (0.99 <= total_weight <= 1.01):  # TolÃ©rance arrondi
            raise ValueError(f"Poids facteurs doivent sommer Ã  1.0, got {total_weight}")
        return v

class DecisionOutcome(BaseModel):
    """RÃ©sultat d'une dÃ©cision d'intervention."""
    decision_id: str
    agent_id: str
    outcome_type: str = Field(..., regex="^(success|ignored|conflict|inappropriate)$")
    quality_score: Optional[float] = Field(None, ge=0, le=1)
    feedback: Optional[str] = None
    improvement_suggestions: List[str] = Field(default_factory=list)
    recorded_at: datetime = Field(default_factory=datetime.utcnow)
```

---

## ğŸ”§ OUTILS ET INTÃ‰GRATIONS

### Integration ChromaDB pour MÃ©moire
```python
# services/memory_service.py
import chromadb
from chromadb.config import Settings
import numpy as np
from typing import List, Dict, Any, Optional

class MemoryService:
    """Service gestion mÃ©moire agents avec ChromaDB."""
    
    def __init__(self, chromadb_url: str):
        self.client = chromadb.HttpClient(host=chromadb_url)
        self.collections = {}
        
    async def initialize_agent_memory(self, agent_id: str) -> None:
        """Initialise mÃ©moire pour un agent."""
        collection_name = f"agent_{agent_id}_memory"
        
        self.collections[agent_id] = self.client.create_collection(
            name=collection_name,
            metadata={"description": f"Memory for agent {agent_id}"}
        )
        
    async def store_conversation(
        self, 
        agent_id: str, 
        conversation: Dict[str, Any],
        embedding: Optional[List[float]] = None
    ) -> str:
        """Stocke conversation dans mÃ©moire agent."""
        
        collection = self.collections.get(agent_id)
        if not collection:
            await self.initialize_agent_memory(agent_id)
            collection = self.collections[agent_id]
            
        # GÃ©nÃ©ration embedding si pas fourni
        if not embedding:
            embedding = await self._generate_embedding(conversation['content'])
            
        # Stockage avec mÃ©tadonnÃ©es
        doc_id = f"conv_{conversation['timestamp']}"
        collection.add(
            embeddings=[embedding],
            documents=[conversation['content']],
            metadatas=[{
                'agent_id': agent_id,
                'channel_id': conversation['channel_id'],
                'timestamp': conversation['timestamp'],
                'participants': conversation['participants'],
                'topic': conversation.get('topic', ''),
                'quality_score': conversation.get('quality_score', 0.5)
            }],
            ids=[doc_id]
        )
        
        return doc_id
        
    async def retrieve_relevant_memories(
        self, 
        agent_id: str, 
        query: str, 
        n_results: int = 5,
        filters: Optional[Dict[str, Any]] = None
    ) -> List[Dict[str, Any]]:
        """RÃ©cupÃ¨re souvenirs pertinents pour contexte."""
        
        collection = self.collections.get(agent_id)
        if not collection:
            return []
            
        # GÃ©nÃ©ration embedding pour requÃªte
        query_embedding = await self._generate_embedding(query)
        
        # Construction filtres ChromaDB
        where_clause = {}
        if filters:
            where_clause.update(filters)
            
        # Recherche similaritÃ©
        results = collection.query(
            query_embeddings=[query_embedding],
            n_results=n_results,
            where=where_clause,
            include=['documents', 'metadatas', 'distances']
        )
        
        # Formatage rÃ©sultats
        memories = []
        for i, doc in enumerate(results['documents'][0]):
            memories.append({
                'content': doc,
                'metadata': results['metadatas'][0][i],
                'similarity': 1 - results['distances'][0][i],  # Distance -> similaritÃ©
                'relevance_score': self._calculate_memory_relevance(
                    results['metadatas'][0][i], 
                    query
                )
            })
            
        return sorted(memories, key=lambda x: x['relevance_score'], reverse=True)
        
    async def _generate_embedding(self, text: str) -> List[float]:
        """GÃ©nÃ¨re embedding pour texte."""
        # IntÃ©gration avec sentence-transformers local
        # Ã€ implÃ©menter selon modÃ¨le embedding choisi
        pass
        
    def _calculate_memory_relevance(
        self, 
        memory_metadata: Dict[str, Any], 
        current_context: str
    ) -> float:
        """Calcule pertinence souvenir pour contexte actuel."""
        # Facteurs : rÃ©cence, similaritÃ© sujet, qualitÃ©, participants communs
        relevance_factors = {
            'recency': self._calculate_recency_factor(memory_metadata['timestamp']),
            'topic_similarity': self._calculate_topic_similarity(
                memory_metadata.get('topic', ''), 
                current_context
            ),
            'quality': memory_metadata.get('quality_score', 0.5),
            'context_match': self._calculate_context_match(
                memory_metadata, 
                current_context
            )
        }
        
        # Score pondÃ©rÃ©
        weights = {'recency': 0.2, 'topic_similarity': 0.4, 'quality': 0.2, 'context_match': 0.2}
        return sum(factor * weights[name] for name, factor in relevance_factors.items())
```

### Integration Ollama OptimisÃ©e
```python
# services/ollama_service.py
import aiohttp
import asyncio
import json
from typing import Dict, Any, List, Optional, AsyncGenerator
import logging

logger = logging.getLogger(__name__)

class OllamaService:
    """Service optimisÃ© pour interactions Ollama."""
    
    def __init__(self, base_url: str = "http://ollama:11434"):
        self.base_url = base_url
        self.session: Optional[aiohttp.ClientSession] = None
        self.model_cache: Dict[str, Any] = {}
        
    async def __aenter__(self):
        self.session = aiohttp.ClientSession(
            timeout=aiohttp.ClientTimeout(total=300),  # 5 min timeout
            connector=aiohttp.TCPConnector(limit=10)
        )
        return self
        
    async def __aexit__(self, exc_type, exc_val, exc_tb):
        if self.session:
            await self.session.close()
            
    async def generate_response(
        self,
        model: str,
        prompt: str,
        system_prompt: Optional[str] = None,
        temperature: float = 0.7,
        max_tokens: Optional[int] = None,
        stream: bool = False
    ) -> Dict[str, Any]:
        """GÃ©nÃ¨re rÃ©ponse via Ollama."""
        
        payload = {
            "model": model,
            "prompt": prompt,
            "stream": stream,
            "options": {
                "temperature": temperature,
                "num_predict": max_tokens or -1
            }
        }
        
        if system_prompt:
            payload["system"] = system_prompt
            
        try:
            async with self.session.post(
                f"{self.base_url}/api/generate",
                json=payload
            ) as response:
                
                if response.status != 200:
                    error_text = await response.text()
                    raise OllamaError(f"Ollama error {response.status}: {error_text}")
                    
                if stream:
                    return await self._handle_streaming_response(response)
                else:
                    result = await response.json()
                    return {
                        "response": result.get("response", ""),
                        "model": model,
                        "done": result.get("done", False),
                        "context": result.get("context", []),
                        "total_duration": result.get("total_duration", 0),
                        "load_duration": result.get("load_duration", 0),
                        "prompt_eval_count": result.get("prompt_eval_count", 0),
                        "eval_count": result.get("eval_count", 0)
                    }
                    
        except asyncio.TimeoutError:
            logger.error(f"Timeout calling Ollama for model {model}")
            raise OllamaTimeoutError(f"Timeout after 300s for model {model}")
        except Exception as e:
            logger.error(f"Error calling Ollama: {e}", exc_info=True)
            raise OllamaError(f"Failed to generate response: {e}")
            
    async def _handle_streaming_response(
        self, 
        response: aiohttp.ClientResponse
    ) -> AsyncGenerator[Dict[str, Any], None]:
        """GÃ¨re rÃ©ponse streaming Ollama."""
        async for line in response.content:
            if line:
                try:
                    data = json.loads(line.decode('utf-8'))
                    yield data
                except json.JSONDecodeError:
                    continue
                    
    async def list_models(self) -> List[Dict[str, Any]]:
        """Liste modÃ¨les disponibles."""
        try:
            async with self.session.get(f"{self.base_url}/api/tags") as response:
                if response.status == 200:
                    result = await response.json()
                    return result.get("models", [])
                else:
                    logger.error(f"Failed to list models: {response.status}")
                    return []
        except Exception as e:
            logger.error(f"Error listing models: {e}")
            return []
            
    async def load_model(self, model: str) -> bool:
        """Charge modÃ¨le en mÃ©moire si pas dÃ©jÃ  fait."""
        if model in self.model_cache:
            return True
            
        try:
            # Test gÃ©nÃ©ration simple pour charger modÃ¨le
            await self.generate_response(
                model=model,
                prompt="Hello",
                max_tokens=1
            )
            self.model_cache[model] = True
            return True
        except Exception as e:
            logger.error(f"Failed to load model {model}: {e}")
            return False
            
    async def health_check(self) -> Dict[str, Any]:
        """VÃ©rification santÃ© service Ollama."""
        try:
            async with self.session.get(f"{self.base_url}/api/tags") as response:
                return {
                    "status": "healthy" if response.status == 200 else "unhealthy",
                    "response_time": response.headers.get("X-Response-Time", "N/A"),
                    "models_available": len(await self.list_models())
                }
        except Exception as e:
            return {
                "status": "unhealthy",
                "error": str(e)
            }

class OllamaError(Exception):
    """Exception Ollama gÃ©nÃ©rale."""
    pass

class OllamaTimeoutError(OllamaError):
    """Exception timeout Ollama."""
    pass
```

---

## ğŸ“Š MONITORING ET DEBUGGING

### Logging StructurÃ©
```python
# core/logging_config.py
import logging
import json
from datetime import datetime
from typing import Dict, Any
import sys

class StructuredLogger:
    """Logger structurÃ© pour debugging et monitoring."""
    
    def __init__(self, name: str, level: str = "INFO"):
        self.logger = logging.getLogger(name)
        self.logger.setLevel(getattr(logging, level.upper()))
        
        # Handler console avec format structurÃ©
        handler = logging.StreamHandler(sys.stdout)
        handler.setFormatter(StructuredFormatter())
        self.logger.addHandler(handler)
        
    def debug(self, message: str, **kwargs):
        self._log("DEBUG", message, **kwargs)
        
    def info(self, message: str, **kwargs):
        self._log("INFO", message, **kwargs)
        
    def warning(self, message: str, **kwargs):
        self._log("WARNING", message, **kwargs)
        
    def error(self, message: str, **kwargs):
        self._log("ERROR", message, **kwargs)
        
    def _log(self, level: str, message: str, **kwargs):
        extra = {
            "timestamp": datetime.utcnow().isoformat(),
            "level": level,
            "message": message,
            **kwargs
        }
        self.logger.log(getattr(logging, level), json.dumps(extra))

class StructuredFormatter(logging.Formatter):
    """Formateur pour logs JSON structurÃ©s."""
    
    def format(self, record):
        if hasattr(record, 'msg') and record.msg.startswith('{'):
            # DÃ©jÃ  formatÃ© JSON
            return record.msg
        else:
            # Format standard â†’ JSON
            return json.dumps({
                "timestamp": datetime.utcnow().isoformat(),
                "level": record.levelname,
                "logger": record.name,
                "message": record.getMessage(),
                "module": record.module,
                "function": record.funcName,
                "line": record.lineno
            })

# Usage dans services
logger = StructuredLogger("relevance-engine")

# Exemples logging
logger.info("Calculating relevance score", 
            agent_id="agent-123", 
            channel_id="channel-456", 
            factors_count=5)

logger.error("Failed to retrieve agent state", 
             agent_id="agent-123", 
             error="AgentNotFound", 
             stack_trace=traceback.format_exc())
```

### MÃ©triques Prometheus
```python
# monitoring/metrics.py
from prometheus_client import Counter, Histogram, Gauge, Info
import time
from functools import wraps
from typing import Callable, Any

# MÃ©triques globales
REQUEST_COUNT = Counter(
    'almaa_requests_total',
    'Total requests by endpoint and method',
    ['method', 'endpoint', 'status']
)

REQUEST_DURATION = Histogram(
    'almaa_request_duration_seconds',
    'Request duration in seconds',
    ['method', 'endpoint']
)

AGENT_STATES = Gauge(
    'almaa_agent_states',
    'Number of agents by state',
    ['state']
)

RELEVANCE_SCORES = Histogram(
    'almaa_relevance_scores',
    'Distribution of relevance scores',
    ['agent_type']
)

OLLAMA_REQUESTS = Counter(
    'almaa_ollama_requests_total',
    'Total requests to Ollama',
    ['model', 'status']
)

OLLAMA_DURATION = Histogram(
    'almaa_ollama_duration_seconds',
    'Ollama request duration',
    ['model']
)

MEMORY_OPERATIONS = Counter(
    'almaa_memory_operations_total',
    'Memory operations',
    ['operation', 'agent_id']
)

# DÃ©corateurs pour mÃ©triques automatiques
def track_requests(endpoint: str = None):
    """DÃ©corateur pour tracker requÃªtes HTTP."""
    def decorator(func: Callable) -> Callable:
        @wraps(func)
        async def wrapper(*args, **kwargs):
            endpoint_name = endpoint or func.__name__
            start_time = time.time()
            
            try:
                result = await func(*args, **kwargs)
                REQUEST_COUNT.labels(
                    method='POST',  # Ã€ adapter selon contexte
                    endpoint=endpoint_name,
                    status='success'
                ).inc()
                return result
            except Exception as e:
                REQUEST_COUNT.labels(
                    method='POST',
                    endpoint=endpoint_name,
                    status='error'
                ).inc()
                raise
            finally:
                REQUEST_DURATION.labels(
                    method='POST',
                    endpoint=endpoint_name
                ).observe(time.time() - start_time)
                
        return wrapper
    return decorator

def track_ollama_calls(model: str):
    """DÃ©corateur pour tracker appels Ollama."""
    def decorator(func: Callable) -> Callable:
        @wraps(func)
        async def wrapper(*args, **kwargs):
            start_time = time.time()
            
            try:
                result = await func(*args, **kwargs)
                OLLAMA_REQUESTS.labels(model=model, status='success').inc()
                return result
            except Exception:
                OLLAMA_REQUESTS.labels(model=model, status='error').inc()
                raise
            finally:
                OLLAMA_DURATION.labels(model=model).observe(time.time() - start_time)
                
        return wrapper
    return decorator

# Exemple usage
@track_requests('calculate_relevance')
async def calculate_relevance_score(agent_id: str, context: Any) -> float:
    # Logique calcul...
    score = 0.75
    RELEVANCE_SCORES.labels(agent_type='developer').observe(score)
    return score
```

---

## âš ï¸ SÃ‰CURITÃ‰ ET VALIDATION

### Validation Inputs
```python
# core/validation.py
from pydantic import BaseModel, validator, ValidationError
from typing import Any, Dict, List, Optional
import re
import html

class InputValidator:
    """Validateur inputs sÃ©curisÃ©."""
    
    @staticmethod
    def sanitize_string(value: str) -> str:
        """Sanitise chaÃ®ne de caractÃ¨res."""
        if not isinstance(value, str):
            raise ValueError("Input must be string")
            
        # HTML escaping
        sanitized = html.escape(value)
        
        # Suppression caractÃ¨res dangereux
        sanitized = re.sub(r'[<>"\']', '', sanitized)
        
        # Limitation longueur
        if len(sanitized) > 10000:
            raise ValueError("String too long (max 10000 chars)")
            
        return sanitized.strip()
    
    @staticmethod
    def validate_agent_id(agent_id: str) -> str:
        """Valide format ID agent."""
        if not re.match(r'^[a-zA-Z0-9_-]{1,50}$', agent_id):
            raise ValueError("Invalid agent ID format")
        return agent_id
    
    @staticmethod
    def validate_model_name(model: str) -> str:
        """Valide nom modÃ¨le Ollama."""
        allowed_models = [
            "llama3.1:8b", "codellama:7b", "mistral:7b", 
            "phi3:3.8b", "gemma:2b", "qwen:4b"
        ]
        if model not in allowed_models:
            raise ValueError(f"Model {model} not allowed")
        return model
    
    @staticmethod
    def validate_file_path(path: str) -> str:
        """Valide chemin fichier sÃ©curisÃ©."""
        # EmpÃªche path traversal
        if '..' in path or path.startswith('/'):
            raise ValueError("Invalid file path")
            
        # CaractÃ¨res autorisÃ©s
        if not re.match(r'^[a-zA-Z0-9_/.-]+$', path):
            raise ValueError("Invalid characters in path")
            
        return path

class SecureRequest(BaseModel):
    """Base pour requÃªtes sÃ©curisÃ©es."""
    
    class Config:
        validate_assignment = True
        extra = "forbid"  # EmpÃªche champs supplÃ©mentaires
        
    @validator('*', pre=True)
    def sanitize_strings(cls, v):
        if isinstance(v, str):
            return InputValidator.sanitize_string(v)
        return v
```

### Sandbox ExÃ©cution
```python
# security/sandbox.py
import docker
import tempfile
import os
import shutil
from typing import Dict, Any, Optional
import asyncio
from pathlib import Path

class DockerSandbox:
    """Sandbox Docker pour exÃ©cution sÃ©curisÃ©e outils."""
    
    def __init__(self):
        self.client = docker.from_env()
        self.base_image = "python:3.11-alpine"
        self.network_name = "almaa_sandbox"
        
    async def execute_tool(
        self,
        tool_name: str,
        code: str,
        files: Dict[str, str] = None,
        env_vars: Dict[str, str] = None,
        timeout: int = 300
    ) -> Dict[str, Any]:
        """ExÃ©cute outil dans sandbox sÃ©curisÃ©."""
        
        # CrÃ©ation environnement temporaire
        with tempfile.TemporaryDirectory() as tmp_dir:
            workspace = Path(tmp_dir)
            
            # Ã‰criture fichiers
            if files:
                for filename, content in files.items():
                    file_path = workspace / filename
                    file_path.parent.mkdir(parents=True, exist_ok=True)
                    file_path.write_text(content)
            
            # Script principal
            main_script = workspace / "main.py"
            main_script.write_text(code)
            
            # Configuration container
            container_config = {
                'image': self.base_image,
                'command': f'python /workspace/main.py',
                'working_dir': '/workspace',
                'volumes': {
                    str(workspace): {
                        'bind': '/workspace',
                        'mode': 'rw'
                    }
                },
                'environment': env_vars or {},
                'network_mode': self.network_name,
                'mem_limit': '512m',
                'cpu_quota': 50000,  # 50% CPU
                'security_opt': ['no-new-privileges'],
                'cap_drop': ['ALL'],
                'read_only': True,
                'tmpfs': {'/tmp': 'noexec,nosuid,size=100m'},
                'detach': True,
                'remove': True
            }
            
            try:
                # ExÃ©cution container
                container = self.client.containers.run(**container_config)
                
                # Attente avec timeout
                result = await asyncio.wait_for(
                    self._wait_container_completion(container),
                    timeout=timeout
                )
                
                return result
                
            except asyncio.TimeoutError:
                # Timeout â†’ kill container
                try:
                    container.kill()
                except:
                    pass
                return {
                    'success': False,
                    'error': f'Tool execution timeout after {timeout}s',
                    'stdout': '',
                    'stderr': '',
                    'exit_code': -1
                }
            except Exception as e:
                return {
                    'success': False,
                    'error': f'Sandbox execution error: {e}',
                    'stdout': '',
                    'stderr': '',
                    'exit_code': -1
                }
    
    async def _wait_container_completion(self, container) -> Dict[str, Any]:
        """Attend complÃ©tion container et rÃ©cupÃ¨re rÃ©sultats."""
        
        # Attente fin exÃ©cution
        exit_code = container.wait()['StatusCode']
        
        # RÃ©cupÃ©ration logs
        logs = container.logs()
        stdout = logs.decode('utf-8', errors='ignore')
        
        # RÃ©cupÃ©ration fichiers output si existants
        output_files = {}
        try:
            # Exemple : rÃ©cupÃ©ration fichier rÃ©sultats
            output_tar = container.get_archive('/workspace/output')[0]
            # Processing tar si nÃ©cessaire...
        except:
            pass
        
        return {
            'success': exit_code == 0,
            'exit_code': exit_code,
            'stdout': stdout,
            'stderr': '',  # Docker combine stdout/stderr
            'output_files': output_files,
            'execution_time': 0  # Ã€ calculer si nÃ©cessaire
        }
    
    def setup_network(self):
        """Configure rÃ©seau isolÃ© pour sandboxes."""
        try:
            # RÃ©seau sans accÃ¨s internet
            self.client.networks.create(
                name=self.network_name,
                driver="bridge",
                internal=True,  # Pas d'accÃ¨s externe
                options={
                    "com.docker.network.bridge.enable_icc": "false"  # Isolation inter-containers
                }
            )
        except docker.errors.APIError as e:
            if "already exists" not in str(e):
                raise
```

---

## ğŸš€ DÃ‰PLOIEMENT ET TESTS

### Scripts de DÃ©veloppement
```bash
#!/bin/bash
# scripts/dev-setup.sh

set -e

echo "ğŸš€ Setting up ALMAA Workspace development environment..."

# VÃ©rification Docker
if ! command -v docker &> /dev/null; then
    echo "âŒ Docker required but not installed"
    exit 1
fi

if ! command -v docker-compose &> /dev/null; then
    echo "âŒ Docker Compose required but not installed"
    exit 1
fi

# CrÃ©ation rÃ©seaux Docker
echo "ğŸ“¡ Creating Docker networks..."
docker network create almaa-network 2>/dev/null || echo "Network already exists"
docker network create almaa-sandbox --internal 2>/dev/null || echo "Sandbox network already exists"

# Variables environnement
if [ ! -f .env ]; then
    echo "âš™ï¸ Creating environment configuration..."
    cat > .env << EOF
# Database
DB_PASSWORD=$(openssl rand -base64 32)
POSTGRES_USER=almaa
POSTGRES_DB=almaa

# MinIO
MINIO_ACCESS_KEY=almaa-admin
MINIO_SECRET_KEY=$(openssl rand -base64 32)

# Monitoring
GRAFANA_PASSWORD=$(openssl rand -base64 16)

# Security
JWT_SECRET_KEY=$(openssl rand -base64 64)
ENCRYPTION_KEY=$(openssl rand -base64 32)

# Development
DEBUG=true
LOG_LEVEL=DEBUG
ENVIRONMENT=development
EOF
    echo "âœ… Environment configuration created (.env)"
fi

# GÃ©nÃ©ration certificats SSL auto-signÃ©s
if [ ! -d "nginx/ssl" ]; then
    echo "ğŸ” Generating SSL certificates..."
    mkdir -p nginx/ssl
    openssl req -x509 -nodes -days 365 -newkey rsa:2048 \
        -keyout nginx/ssl/almaa.key \
        -out nginx/ssl/almaa.crt \
        -subj "/C=FR/ST=Local/L=Local/O=ALMAA/CN=almaa.local"
    echo "âœ… SSL certificates generated"
fi

# Build images dÃ©veloppement
echo "ğŸ—ï¸ Building development images..."
docker-compose -f docker-compose.dev.yml build

# Initialisation base de donnÃ©es
echo "ğŸ—ƒï¸ Initializing database..."
docker-compose -f docker-compose.dev.yml up -d postgres redis
sleep 10

# Migrations
docker-compose -f docker-compose.dev.yml exec postgres psql -U almaa -d almaa -c "
CREATE EXTENSION IF NOT EXISTS \"uuid-ossp\";
CREATE EXTENSION IF NOT EXISTS \"pgvector\";
"

# Test connexions
echo "ğŸ” Testing connections..."
docker-compose -f docker-compose.dev.yml up -d ollama chromadb minio
sleep 15

# Validation Ollama
echo "Testing Ollama..."
curl -sf http://localhost:11434/api/tags > /dev/null && echo "âœ… Ollama OK" || echo "âŒ Ollama failed"

# Validation ChromaDB
echo "Testing ChromaDB..."
curl -sf http://localhost:8000/api/v1/heartbeat > /dev/null && echo "âœ… ChromaDB OK" || echo "âŒ ChromaDB failed"

# Validation MinIO
echo "Testing MinIO..."
curl -sf http://localhost:9001 > /dev/null && echo "âœ… MinIO OK" || echo "âŒ MinIO failed"

# DÃ©marrage complet
echo "ğŸš€ Starting complete development environment..."
docker-compose -f docker-compose.dev.yml up -d

echo ""
echo "ğŸ‰ Development environment ready!"
echo "ğŸ“Š Admin Dashboard: https://localhost (admin/admin)"
echo "ğŸ“Š Grafana: http://localhost:3000 (admin/$(grep GRAFANA_PASSWORD .env | cut -d'=' -f2))"
echo "ğŸ—ƒï¸ MinIO Console: http://localhost:9001 (almaa-admin/SECRET)"
echo "ğŸ”§ API Docs: http://localhost:8000/docs"
echo ""
echo "ğŸ“ Next steps:"
echo "1. Load Ollama models: docker-compose exec ollama ollama pull llama3.1:8b"
echo "2. Run tests: ./scripts/run-tests.sh"
echo "3. View logs: docker-compose logs -f"
```

### Tests AutomatisÃ©s
```bash
#!/bin/bash
# scripts/run-tests.sh

set -e

echo "ğŸ§ª Running ALMAA Workspace test suite..."

# Tests unitaires Python
echo "ğŸ“‹ Running Python unit tests..."
docker-compose -f docker-compose.test.yml run --rm api pytest tests/unit/ -v --cov=src --cov-report=term-missing

# Tests intÃ©gration
echo "ğŸ“‹ Running integration tests..."
docker-compose -f docker-compose.test.yml run --rm api pytest tests/integration/ -v

# Tests frontend
echo "ğŸ“‹ Running frontend tests..."
docker-compose -f docker-compose.test.yml run --rm frontend npm test

# Tests end-to-end
echo "ğŸ“‹ Running E2E tests..."
docker-compose -f docker-compose.test.yml run --rm e2e-tests

# VÃ©rifications sÃ©curitÃ©
echo "ğŸ”’ Running security checks..."
docker-compose -f docker-compose.test.yml run --rm security-scanner

# Tests performance
echo "âš¡ Running performance tests..."
docker-compose -f docker-compose.test.yml run --rm load-tests

# Nettoyage
echo "ğŸ§¹ Cleaning up test environment..."
docker-compose -f docker-compose.test.yml down -v

echo "âœ… All tests passed!"
```

---

## ğŸ“– CONCLUSION POUR GPT-5

Ce guide complet te donne tous les Ã©lÃ©ments nÃ©cessaires pour dÃ©velopper ALMAA Workspace V2.0 avec succÃ¨s :

### ğŸ¯ **PrioritÃ©s Absolues**
1. **SystÃ¨me de Pertinence** â†’ Agents parlent quand c'est pertinent
2. **Interface Admin** â†’ ContrÃ´le complet des agents
3. **Templates Agents** â†’ SpÃ©cialisation et configuration
4. **Communication Inter-Agents** â†’ MPs et canaux deepthink

### ğŸ› ï¸ **Standards QualitÃ©**
- Code documentÃ© et testÃ©
- Validation sÃ©curisÃ©e de tous inputs
- Logging structurÃ© pour debugging
- MÃ©triques Prometheus intÃ©grÃ©es
- Configuration flexible YAML

### ğŸš€ **Approche RecommandÃ©e**
1. **Analyse Code Existant** â†’ Comprendre l'architecture actuelle
2. **DÃ©veloppement IncrÃ©mental** â†’ Extensions pas reconstruction
3. **Tests Continus** â†’ Validation Ã  chaque Ã©tape
4. **Documentation** â†’ Explique chaque choix technique

### ğŸ’¬ **Communication**
- Pose des questions sur l'architecture existante
- Explique tes choix techniques
- Signale les compromis ou limitations
- Propose des amÃ©liorations d'architecture

**Ensemble, nous allons crÃ©er l'assistant AGI personnel le plus avancÃ© et sÃ©curisÃ© possible ! ğŸš€**