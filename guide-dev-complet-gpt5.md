# üìñ GUIDE D√âVELOPPEMENT COMPLET ALMAA WORKSPACE - POUR GPT-5

## üéØ CONTEXTE ET OBJECTIFS

### Situation Actuelle
Tu d√©veloppes avec moi la **Phase 4** du projet ALMAA Workspace, une √©volution majeure d'un prototype Discord IA existant vers un assistant AGI personnel complet.

**Objectif Principal** : Cr√©er un syst√®me multi-agents sp√©cialis√©s avec :
- Syst√®me de pertinence contextuelle (agents parlent quand c'est pertinent)
- Interface admin compl√®te avec contr√¥le des agents
- Outils modulaires pour productivit√©
- Gouvernance IA avec mod√©ration autonome
- Architecture scalable pour 100+ agents

### Utilisateur Final
- **Profil** : Chef de projet en arr√™t maladie, budget 200k‚Ç¨, serveur 9600 TOPS pr√©vu
- **Objectif** : Assistant AGI personnel pour tous projets (d√©veloppement, √©criture, cr√©ativit√©, analyse)
- **Contraintes** : 100% offline, s√©curit√© maximale, temps illimit√© de d√©veloppement
- **Use Case #1** : Agents l'aident √† d√©velopper le projet ALMAA lui-m√™me

---

## üìã INSTRUCTIONS SP√âCIFIQUES D√âVELOPPEMENT

### ‚ö†Ô∏è R√àGLES CRITIQUES √Ä RESPECTER

1. **Base Existante** : Tu pars du code existant GitHub (architecture d√©j√† en place)
2. **Approche Incr√©mentale** : Am√©liore le code existant, pas de reconstruction compl√®te
3. **Analyse Avant Action** : Toujours analyser le code existant avant de proposer modifications
4. **Documentation Compl√®te** : Documente chaque modification avec commentaires d√©taill√©s
5. **Tests Inclus** : Inclus tests pour chaque nouvelle fonctionnalit√©
6. **Configuration Flexible** : Rends tout configurable via fichiers YAML/JSON
7. **Performance** : Optimise pour temps r√©el et charge √©lev√©e
8. **S√©curit√©** : Valide toutes les entr√©es, sandboxe les ex√©cutions

### üõ†Ô∏è STACK TECHNIQUE EXISTANTE
```yaml
Backend:
  api: "FastAPI + Uvicorn"
  database: "PostgreSQL 16 + pgvector"
  cache: "Redis 7 Alpine"
  vector_db: "ChromaDB"
  storage: "MinIO S3-compatible"
  ai: "Ollama + mod√®les locaux"
  
Frontend:
  framework: "Next.js (Static Site Generation)"
  ui: "React + TypeScript"
  styling: "Tailwind CSS"
  state: "React hooks + Context"
  
Infrastructure:
  proxy: "Nginx Alpine"
  containers: "Docker Compose"
  monitoring: "Prometheus + Grafana"
  logging: "Loki + Grafana"
```

---

## üéØ ROADMAP D√âVELOPPEMENT PRIORITAIRE

### PHASE 1 : FOUNDATION (Semaines 1-4) - PRIORIT√â CRITIQUE

#### Semaine 1-2 : Syst√®me de Pertinence Contextuelle
**Objectif** : Agents parlent seulement quand c'est pertinent (score > 0.5)

**D√©veloppements requis :**

1. **Service Relevance Engine**
```python
# √Ä cr√©er : relevance-engine/
‚îú‚îÄ‚îÄ src/
‚îÇ   ‚îú‚îÄ‚îÄ relevance_calculator.py    # Calcul scores pertinence
‚îÇ   ‚îú‚îÄ‚îÄ context_analyzer.py        # Analyse contexte conversation
‚îÇ   ‚îú‚îÄ‚îÄ decision_engine.py         # D√©cision intervention agent
‚îÇ   ‚îî‚îÄ‚îÄ learning_system.py         # Apprentissage patterns
‚îú‚îÄ‚îÄ models/
‚îÇ   ‚îî‚îÄ‚îÄ relevance_factors.py       # Facteurs pertinence
‚îú‚îÄ‚îÄ api/
‚îÇ   ‚îî‚îÄ‚îÄ relevance_api.py           # API REST endpoints
‚îî‚îÄ‚îÄ config/
    ‚îî‚îÄ‚îÄ relevance_config.yaml      # Configuration facteurs
```

**Facteurs de pertinence √† impl√©menter :**
- `expertise_match` : Correspondance expertise agent / sujet conversation
- `conversation_gap` : D√©tection manque d'information dans conversation
- `workload_capacity` : Charge de travail actuelle agent
- `recent_participation` : √âviter participation r√©p√©t√©e
- `relationship_trust` : Niveau confiance avec autres agents
- `timing_appropriateness` : Moment appropri√© pour intervenir

2. **Int√©gration WebSocket pour Temps R√©el**
```python
# Extension WebSocket existant
websocket_manager.py:
  - add_relevance_checking()
  - integrate_decision_engine()
  - real_time_score_updates()
```

3. **Extension API FastAPI**
```python
# Nouveaux endpoints √† ajouter
/api/v1/relevance/
‚îú‚îÄ‚îÄ POST /calculate          # Calcul score pertinence
‚îú‚îÄ‚îÄ GET  /scores/{agent_id}  # Scores agent
‚îú‚îÄ‚îÄ PUT  /thresholds         # Mise √† jour seuils
‚îî‚îÄ‚îÄ GET  /analytics          # Analytics pertinence
```

#### Semaine 3-4 : Interface Admin Basique
**Objectif** : Dashboard avec contr√¥les agents essentiels

**D√©veloppements requis :**

1. **Dashboard Admin (Next.js)**
```typescript
// Composants √† cr√©er
components/admin/
‚îú‚îÄ‚îÄ AgentControlPanel.tsx      # Panneau contr√¥le principal
‚îú‚îÄ‚îÄ AgentCard.tsx              # Carte agent individuel
‚îú‚îÄ‚îÄ PauseResumeButton.tsx      # Contr√¥les pause/reprendre
‚îú‚îÄ‚îÄ TaskInjector.tsx           # Interface injection t√¢ches
‚îú‚îÄ‚îÄ DebugMonitor.tsx           # Monitoring debug temps r√©el
‚îú‚îÄ‚îÄ NotificationCenter.tsx     # Centre notifications
‚îî‚îÄ‚îÄ SystemMetrics.tsx          # M√©triques syst√®me

pages/admin/
‚îú‚îÄ‚îÄ dashboard.tsx              # Dashboard principal
‚îú‚îÄ‚îÄ agents.tsx                 # Gestion agents
‚îî‚îÄ‚îÄ debug.tsx                  # Interface debug
```

2. **API Extensions**
```python
# Nouveaux endpoints admin
/api/v1/admin/
‚îú‚îÄ‚îÄ POST /agents/{id}/pause    # Pause agent
‚îú‚îÄ‚îÄ POST /agents/{id}/resume   # Reprendre agent
‚îú‚îÄ‚îÄ POST /tasks/inject         # Injection t√¢che prioritaire
‚îú‚îÄ‚îÄ GET  /debug/{agent_id}     # Debug infos agent
‚îî‚îÄ‚îÄ GET  /notifications        # Notifications admin
```

### PHASE 2 : AGENT MANAGEMENT (Semaines 5-8)

#### Templates d'Agents Pr√©-configur√©s
**Objectif** : Templates sp√©cialis√©s (Developer, Analyst, Moderator, etc.)

**Templates essentiels :**
```yaml
Developer Backend:
  model: "codellama:7b"
  context_size: 8192
  personality: "Rigoureux, m√©thodique, orient√© solution"
  tools: ["git", "code-analysis", "testing", "docker"]
  system_prompt: "Tu es un expert d√©veloppeur backend Python/FastAPI..."
  
Analyst Data:
  model: "llama3.1:8b"
  context_size: 16384
  personality: "Analytique, pr√©cis, synth√©tique"
  tools: ["data-processing", "visualization", "research"]
  system_prompt: "Tu es un expert en analyse de donn√©es..."
  
Moderator Quality:
  model: "mistral:7b"
  context_size: 8192
  personality: "√âquilibr√©, juste, diplomatique"
  tools: ["moderation", "quality-check", "conflict-resolution"]
  system_prompt: "Tu es un superviseur qualit√© et m√©diateur..."
```

#### Communication Inter-Agents
**Objectif** : Messages priv√©s entre agents + canaux deepthink

**D√©veloppements requis :**
1. **Messages Priv√©s** : Syst√®me MP avec acceptation/refus
2. **Canaux Deepthink** : Canaux priv√©s r√©flexion individuelle
3. **Archivage Automatique** : Archivage conversations apr√®s 48h inactivit√©

---

## üíª STANDARDS D√âVELOPPEMENT

### Structure Code
```python
# Standard structure pour nouveaux services
service-name/
‚îú‚îÄ‚îÄ src/
‚îÇ   ‚îú‚îÄ‚îÄ __init__.py
‚îÇ   ‚îú‚îÄ‚îÄ main.py                    # Point d'entr√©e service
‚îÇ   ‚îú‚îÄ‚îÄ core/                      # Logique m√©tier core
‚îÇ   ‚îú‚îÄ‚îÄ api/                       # Endpoints API REST
‚îÇ   ‚îú‚îÄ‚îÄ models/                    # Mod√®les donn√©es
‚îÇ   ‚îú‚îÄ‚îÄ services/                  # Services m√©tier
‚îÇ   ‚îú‚îÄ‚îÄ utils/                     # Utilitaires
‚îÇ   ‚îî‚îÄ‚îÄ exceptions/                # Exceptions personnalis√©es
‚îú‚îÄ‚îÄ tests/
‚îÇ   ‚îú‚îÄ‚îÄ unit/                      # Tests unitaires
‚îÇ   ‚îú‚îÄ‚îÄ integration/               # Tests int√©gration
‚îÇ   ‚îî‚îÄ‚îÄ fixtures/                  # Fixtures tests
‚îú‚îÄ‚îÄ config/
‚îÇ   ‚îú‚îÄ‚îÄ settings.yaml              # Configuration service
‚îÇ   ‚îî‚îÄ‚îÄ logging.yaml               # Configuration logs
‚îú‚îÄ‚îÄ docker/
‚îÇ   ‚îú‚îÄ‚îÄ Dockerfile                 # Image Docker
‚îÇ   ‚îî‚îÄ‚îÄ docker-compose.test.yml    # Tests Docker
‚îú‚îÄ‚îÄ requirements/
‚îÇ   ‚îú‚îÄ‚îÄ base.txt                   # D√©pendances base
‚îÇ   ‚îî‚îÄ‚îÄ dev.txt                    # D√©pendances d√©veloppement
‚îî‚îÄ‚îÄ README.md                      # Documentation service
```

### Standards Codage
```python
# Exemple standard fonction
async def calculate_relevance_score(
    agent_id: str,
    conversation_context: ConversationContext,
    current_message: Message
) -> RelevanceScore:
    """
    Calcule le score de pertinence pour qu'un agent intervienne.
    
    Args:
        agent_id: ID unique de l'agent
        conversation_context: Contexte conversation actuelle
        current_message: Message d√©clencheur
        
    Returns:
        RelevanceScore: Score et d√©tails facteurs
        
    Raises:
        AgentNotFoundError: Si agent n'existe pas
        ContextInvalidError: Si contexte invalide
    """
    try:
        # Validation inputs
        await validate_agent_exists(agent_id)
        validate_conversation_context(conversation_context)
        
        # R√©cup√©ration √©tat agent
        agent_state = await get_agent_state(agent_id)
        
        # Calcul facteurs pertinence
        factors = await calculate_factors(
            agent_state=agent_state,
            context=conversation_context,
            message=current_message
        )
        
        # Score final pond√©r√©
        final_score = weighted_sum(factors, agent_state.personality.weights)
        
        # Log pour debug
        logger.debug(
            f"Relevance calculated for agent {agent_id}: {final_score}",
            extra={"agent_id": agent_id, "score": final_score, "factors": factors}
        )
        
        return RelevanceScore(
            agent_id=agent_id,
            score=final_score,
            factors=factors,
            timestamp=datetime.utcnow()
        )
        
    except Exception as e:
        logger.error(f"Error calculating relevance: {e}", exc_info=True)
        raise RelevanceCalculationError(f"Failed to calculate relevance: {e}")
```

### Standards Tests
```python
# Exemple test complet
import pytest
from unittest.mock import AsyncMock, patch
from fastapi.testclient import TestClient

class TestRelevanceCalculator:
    
    @pytest.fixture
    async def sample_agent(self):
        return Agent(
            id="agent-123",
            name="Test Developer",
            type="developer",
            status="active",
            expertise=["python", "fastapi"]
        )
    
    @pytest.fixture
    def sample_conversation(self):
        return ConversationContext(
            channel_id="channel-456",
            recent_messages=[
                Message(content="How to optimize FastAPI performance?"),
                Message(content="We need better database queries")
            ],
            participants=["agent-789", "user-admin"],
            topic="performance optimization"
        )
    
    async def test_calculate_relevance_score_high_match(
        self, sample_agent, sample_conversation
    ):
        """Test score √©lev√© quand expertise correspond parfaitement."""
        
        with patch('relevance_calculator.get_agent_state') as mock_get_state:
            mock_get_state.return_value = sample_agent
            
            result = await calculate_relevance_score(
                agent_id=sample_agent.id,
                conversation_context=sample_conversation,
                current_message=Message(content="FastAPI optimization tips?")
            )
            
            assert result.score > 0.8
            assert "expertise_match" in result.factors
            assert result.factors["expertise_match"] > 0.9
    
    async def test_calculate_relevance_score_low_match(
        self, sample_agent, sample_conversation
    ):
        """Test score faible quand pas d'expertise correspondante."""
        
        off_topic_conversation = ConversationContext(
            channel_id="channel-789",
            recent_messages=[Message(content="Best cooking recipes?")],
            topic="cooking"
        )
        
        with patch('relevance_calculator.get_agent_state') as mock_get_state:
            mock_get_state.return_value = sample_agent
            
            result = await calculate_relevance_score(
                agent_id=sample_agent.id,
                conversation_context=off_topic_conversation,
                current_message=Message(content="Recipe for pasta?")
            )
            
            assert result.score < 0.3
            assert result.factors["expertise_match"] < 0.2
    
    async def test_calculate_relevance_agent_not_found(self):
        """Test erreur quand agent n'existe pas."""
        
        with pytest.raises(AgentNotFoundError):
            await calculate_relevance_score(
                agent_id="non-existent",
                conversation_context=ConversationContext(),
                current_message=Message(content="test")
            )
```

### Standards Configuration
```yaml
# Exemple configuration service
# config/relevance_config.yaml
relevance_engine:
  thresholds:
    intervention: 0.5      # Score minimum pour intervention
    high_priority: 0.8     # Score pour intervention prioritaire
    low_priority: 0.2      # Score pour √©coute passive uniquement
    
  factors:
    expertise_match:
      weight: 0.3
      enabled: true
      min_threshold: 0.1
      
    conversation_gap:
      weight: 0.25
      enabled: true
      detection_method: "semantic_similarity"
      
    workload_capacity:
      weight: 0.2
      enabled: true
      max_concurrent_tasks: 3
      
    recent_participation:
      weight: 0.15
      enabled: true
      cooldown_minutes: 30
      
    timing_appropriateness:
      weight: 0.1
      enabled: true
      avoid_interruption: true
      
  performance:
    cache_ttl: 300        # TTL cache scores (5 minutes)
    calculation_timeout: 5 # Timeout calcul (5 secondes)
    batch_size: 10        # Taille lot pour traitement
    
  logging:
    level: "INFO"
    include_factors: true
    include_scores: true
```

---

## üóÉÔ∏è BASES DE DONN√âES ET MOD√àLES

### Mod√®les PostgreSQL Existants √† √âtendre
```sql
-- Extensions n√©cessaires pour nouveaux mod√®les
CREATE EXTENSION IF NOT EXISTS "uuid-ossp";
CREATE EXTENSION IF NOT EXISTS "pgvector";

-- Nouveau sch√©ma pour pertinence
CREATE SCHEMA IF NOT EXISTS relevance;

-- Table scores de pertinence
CREATE TABLE relevance.scores (
    id UUID PRIMARY KEY DEFAULT uuid_generate_v4(),
    agent_id UUID NOT NULL REFERENCES agents(id),
    conversation_context JSONB NOT NULL,
    score REAL NOT NULL CHECK (score >= 0 AND score <= 1),
    factors JSONB NOT NULL,
    intervention_decision BOOLEAN NOT NULL,
    created_at TIMESTAMP WITH TIME ZONE DEFAULT NOW(),
    
    -- Index pour performance
    INDEX idx_relevance_agent_created (agent_id, created_at DESC),
    INDEX idx_relevance_score_created (score DESC, created_at DESC)
);

-- Table historique d√©cisions
CREATE TABLE relevance.decisions (
    id UUID PRIMARY KEY DEFAULT uuid_generate_v4(),
    agent_id UUID NOT NULL REFERENCES agents(id),
    channel_id UUID NOT NULL REFERENCES channels(id),
    message_id UUID REFERENCES messages(id),
    decision_type VARCHAR(50) NOT NULL, -- 'intervene', 'observe', 'reflect'
    relevance_score REAL NOT NULL,
    factors JSONB NOT NULL,
    outcome VARCHAR(50), -- 'success', 'ignored', 'conflict'
    feedback_score REAL, -- Feedback qualit√© intervention
    created_at TIMESTAMP WITH TIME ZONE DEFAULT NOW()
);
```

### Mod√®les Pydantic
```python
# models/relevance_models.py
from pydantic import BaseModel, Field, validator
from typing import Dict, List, Optional, Any
from datetime import datetime
from enum import Enum

class RelevanceFactor(BaseModel):
    """Facteur individuel de pertinence."""
    name: str = Field(..., description="Nom du facteur")
    value: float = Field(..., ge=0, le=1, description="Valeur du facteur [0-1]")
    weight: float = Field(..., ge=0, le=1, description="Poids du facteur [0-1]")
    explanation: Optional[str] = Field(None, description="Explication facteur")
    metadata: Dict[str, Any] = Field(default_factory=dict)

class ConversationContext(BaseModel):
    """Contexte de conversation pour calcul pertinence."""
    channel_id: str = Field(..., description="ID du canal")
    server_id: str = Field(..., description="ID du serveur")
    recent_messages: List[Dict[str, Any]] = Field(default_factory=list)
    participants: List[str] = Field(default_factory=list)
    topic: Optional[str] = Field(None, description="Sujet conversation")
    urgency_level: float = Field(0.5, ge=0, le=1, description="Niveau urgence")
    conversation_flow: str = Field("normal", description="Type de flow")

class AgentState(BaseModel):
    """√âtat actuel d'un agent."""
    agent_id: str
    status: str = Field(..., regex="^(active|paused|working|error)$")
    current_workload: float = Field(0, ge=0, le=1, description="Charge actuelle")
    expertise_areas: List[str] = Field(default_factory=list)
    performance_score: float = Field(0.5, ge=0, le=1)
    recent_activity: Dict[str, Any] = Field(default_factory=dict)
    personality_weights: Dict[str, float] = Field(default_factory=dict)
    last_intervention: Optional[datetime] = None

class RelevanceScore(BaseModel):
    """Score de pertinence calcul√©."""
    agent_id: str
    score: float = Field(..., ge=0, le=1, description="Score final [0-1]")
    factors: Dict[str, RelevanceFactor] = Field(..., description="D√©tail facteurs")
    decision: str = Field(..., regex="^(intervene|observe|reflect)$")
    confidence: float = Field(..., ge=0, le=1, description="Confiance d√©cision")
    explanation: str = Field(..., description="Explication d√©cision")
    timestamp: datetime = Field(default_factory=datetime.utcnow)
    
    @validator('factors')
    def validate_factors_sum_weights(cls, v):
        total_weight = sum(factor.weight for factor in v.values())
        if not (0.99 <= total_weight <= 1.01):  # Tol√©rance arrondi
            raise ValueError(f"Poids facteurs doivent sommer √† 1.0, got {total_weight}")
        return v

class DecisionOutcome(BaseModel):
    """R√©sultat d'une d√©cision d'intervention."""
    decision_id: str
    agent_id: str
    outcome_type: str = Field(..., regex="^(success|ignored|conflict|inappropriate)$")
    quality_score: Optional[float] = Field(None, ge=0, le=1)
    feedback: Optional[str] = None
    improvement_suggestions: List[str] = Field(default_factory=list)
    recorded_at: datetime = Field(default_factory=datetime.utcnow)
```

---

## üîß OUTILS ET INT√âGRATIONS

### Integration ChromaDB pour M√©moire
```python
# services/memory_service.py
import chromadb
from chromadb.config import Settings
import numpy as np
from typing import List, Dict, Any, Optional

class MemoryService:
    """Service gestion m√©moire agents avec ChromaDB."""
    
    def __init__(self, chromadb_url: str):
        self.client = chromadb.HttpClient(host=chromadb_url)
        self.collections = {}
        
    async def initialize_agent_memory(self, agent_id: str) -> None:
        """Initialise m√©moire pour un agent."""
        collection_name = f"agent_{agent_id}_memory"
        
        self.collections[agent_id] = self.client.create_collection(
            name=collection_name,
            metadata={"description": f"Memory for agent {agent_id}"}
        )
        
    async def store_conversation(
        self, 
        agent_id: str, 
        conversation: Dict[str, Any],
        embedding: Optional[List[float]] = None
    ) -> str:
        """Stocke conversation dans m√©moire agent."""
        
        collection = self.collections.get(agent_id)
        if not collection:
            await self.initialize_agent_memory(agent_id)
            collection = self.collections[agent_id]
            
        # G√©n√©ration embedding si pas fourni
        if not embedding:
            embedding = await self._generate_embedding(conversation['content'])
            
        # Stockage avec m√©tadonn√©es
        doc_id = f"conv_{conversation['timestamp']}"
        collection.add(
            embeddings=[embedding],
            documents=[conversation['content']],
            metadatas=[{
                'agent_id': agent_id,
                'channel_id': conversation['channel_id'],
                'timestamp': conversation['timestamp'],
                'participants': conversation['participants'],
                'topic': conversation.get('topic', ''),
                'quality_score': conversation.get('quality_score', 0.5)
            }],
            ids=[doc_id]
        )
        
        return doc_id
        
    async def retrieve_relevant_memories(
        self, 
        agent_id: str, 
        query: str, 
        n_results: int = 5,
        filters: Optional[Dict[str, Any]] = None
    ) -> List[Dict[str, Any]]:
        """R√©cup√®re souvenirs pertinents pour contexte."""
        
        collection = self.collections.get(agent_id)
        if not collection:
            return []
            
        # G√©n√©ration embedding pour requ√™te
        query_embedding = await self._generate_embedding(query)
        
        # Construction filtres ChromaDB
        where_clause = {}
        if filters:
            where_clause.update(filters)
            
        # Recherche similarit√©
        results = collection.query(
            query_embeddings=[query_embedding],
            n_results=n_results,
            where=where_clause,
            include=['documents', 'metadatas', 'distances']
        )
        
        # Formatage r√©sultats
        memories = []
        for i, doc in enumerate(results['documents'][0]):
            memories.append({
                'content': doc,
                'metadata': results['metadatas'][0][i],
                'similarity': 1 - results['distances'][0][i],  # Distance -> similarit√©
                'relevance_score': self._calculate_memory_relevance(
                    results['metadatas'][0][i], 
                    query
                )
            })
            
        return sorted(memories, key=lambda x: x['relevance_score'], reverse=True)
        
    async def _generate_embedding(self, text: str) -> List[float]:
        """G√©n√®re embedding pour texte."""
        # Int√©gration avec sentence-transformers local
        # √Ä impl√©menter selon mod√®le embedding choisi
        pass
        
    def _calculate_memory_relevance(
        self, 
        memory_metadata: Dict[str, Any], 
        current_context: str
    ) -> float:
        """Calcule pertinence souvenir pour contexte actuel."""
        # Facteurs : r√©cence, similarit√© sujet, qualit√©, participants communs
        relevance_factors = {
            'recency': self._calculate_recency_factor(memory_metadata['timestamp']),
            'topic_similarity': self._calculate_topic_similarity(
                memory_metadata.get('topic', ''), 
                current_context
            ),
            'quality': memory_metadata.get('quality_score', 0.5),
            'context_match': self._calculate_context_match(
                memory_metadata, 
                current_context
            )
        }
        
        # Score pond√©r√©
        weights = {'recency': 0.2, 'topic_similarity': 0.4, 'quality': 0.2, 'context_match': 0.2}
        return sum(factor * weights[name] for name, factor in relevance_factors.items())
```

### Integration Ollama Optimis√©e
```python
# services/ollama_service.py
import aiohttp
import asyncio
import json
from typing import Dict, Any, List, Optional, AsyncGenerator
import logging

logger = logging.getLogger(__name__)

class OllamaService:
    """Service optimis√© pour interactions Ollama."""
    
    def __init__(self, base_url: str = "http://ollama:11434"):
        self.base_url = base_url
        self.session: Optional[aiohttp.ClientSession] = None
        self.model_cache: Dict[str, Any] = {}
        
    async def __aenter__(self):
        self.session = aiohttp.ClientSession(
            timeout=aiohttp.ClientTimeout(total=300),  # 5 min timeout
            connector=aiohttp.TCPConnector(limit=10)
        )
        return self
        
    async def __aexit__(self, exc_type, exc_val, exc_tb):
        if self.session:
            await self.session.close()
            
    async def generate_response(
        self,
        model: str,
        prompt: str,
        system_prompt: Optional[str] = None,
        temperature: float = 0.7,
        max_tokens: Optional[int] = None,
        stream: bool = False
    ) -> Dict[str, Any]:
        """G√©n√®re r√©ponse via Ollama."""
        
        payload = {
            "model": model,
            "prompt": prompt,
            "stream": stream,
            "options": {
                "temperature": temperature,
                "num_predict": max_tokens or -1
            }
        }
        
        if system_prompt:
            payload["system"] = system_prompt
            
        try:
            async with self.session.post(
                f"{self.base_url}/api/generate",
                json=payload
            ) as response:
                
                if response.status != 200:
                    error_text = await response.text()
                    raise OllamaError(f"Ollama error {response.status}: {error_text}")
                    
                if stream:
                    return await self._handle_streaming_response(response)
                else:
                    result = await response.json()
                    return {
                        "response": result.get("response", ""),
                        "model": model,
                        "done": result.get("done", False),
                        "context": result.get("context", []),
                        "total_duration": result.get("total_duration", 0),
                        "load_duration": result.get("load_duration", 0),
                        "prompt_eval_count": result.get("prompt_eval_count", 0),
                        "eval_count": result.get("eval_count", 0)
                    }
                    
        except asyncio.TimeoutError:
            logger.error(f"Timeout calling Ollama for model {model}")
            raise OllamaTimeoutError(f"Timeout after 300s for model {model}")
        except Exception as e:
            logger.error(f"Error calling Ollama: {e}", exc_info=True)
            raise OllamaError(f"Failed to generate response: {e}")
            
    async def _handle_streaming_response(
        self, 
        response: aiohttp.ClientResponse
    ) -> AsyncGenerator[Dict[str, Any], None]:
        """G√®re r√©ponse streaming Ollama."""
        async for line in response.content:
            if line:
                try:
                    data = json.loads(line.decode('utf-8'))
                    yield data
                except json.JSONDecodeError:
                    continue
                    
    async def list_models(self) -> List[Dict[str, Any]]:
        """Liste mod√®les disponibles."""
        try:
            async with self.session.get(f"{self.base_url}/api/tags") as response:
                if response.status == 200:
                    result = await response.json()
                    return result.get("models", [])
                else:
                    logger.error(f"Failed to list models: {response.status}")
                    return []
        except Exception as e:
            logger.error(f"Error listing models: {e}")
            return []
            
    async def load_model(self, model: str) -> bool:
        """Charge mod√®le en m√©moire si pas d√©j√† fait."""
        if model in self.model_cache:
            return True
            
        try:
            # Test g√©n√©ration simple pour charger mod√®le
            await self.generate_response(
                model=model,
                prompt="Hello",
                max_tokens=1
            )
            self.model_cache[model] = True
            return True
        except Exception as e:
            logger.error(f"Failed to load model {model}: {e}")
            return False
            
    async def health_check(self) -> Dict[str, Any]:
        """V√©rification sant√© service Ollama."""
        try:
            async with self.session.get(f"{self.base_url}/api/tags") as response:
                return {
                    "status": "healthy" if response.status == 200 else "unhealthy",
                    "response_time": response.headers.get("X-Response-Time", "N/A"),
                    "models_available": len(await self.list_models())
                }
        except Exception as e:
            return {
                "status": "unhealthy",
                "error": str(e)
            }

class OllamaError(Exception):
    """Exception Ollama g√©n√©rale."""
    pass

class OllamaTimeoutError(OllamaError):
    """Exception timeout Ollama."""
    pass
```

---

## üìä MONITORING ET DEBUGGING

### Logging Structur√©
```python
# core/logging_config.py
import logging
import json
from datetime import datetime
from typing import Dict, Any
import sys

class StructuredLogger:
    """Logger structur√© pour debugging et monitoring."""
    
    def __init__(self, name: str, level: str = "INFO"):
        self.logger = logging.getLogger(name)
        self.logger.setLevel(getattr(logging, level.upper()))
        
        # Handler console avec format structur√©
        handler = logging.StreamHandler(sys.stdout)
        handler.setFormatter(StructuredFormatter())
        self.logger.addHandler(handler)
        
    def debug(self, message: str, **kwargs):
        self._log("DEBUG", message, **kwargs)
        
    def info(self, message: str, **kwargs):
        self._log("INFO", message, **kwargs)
        
    def warning(self, message: str, **kwargs):
        self._log("WARNING", message, **kwargs)
        
    def error(self, message: str, **kwargs):
        self._log("ERROR", message, **kwargs)
        
    def _log(self, level: str, message: str, **kwargs):
        extra = {
            "timestamp": datetime.utcnow().isoformat(),
            "level": level,
            "message": message,
            **kwargs
        }
        self.logger.log(getattr(logging, level), json.dumps(extra))

class StructuredFormatter(logging.Formatter):
    """Formateur pour logs JSON structur√©s."""
    
    def format(self, record):
        if hasattr(record, 'msg') and record.msg.startswith('{'):
            # D√©j√† format√© JSON
            return record.msg
        else:
            # Format standard ‚Üí JSON
            return json.dumps({
                "timestamp": datetime.utcnow().isoformat(),
                "level": record.levelname,
                "logger": record.name,
                "message": record.getMessage(),
                "module": record.module,
                "function": record.funcName,
                "line": record.lineno
            })

# Usage dans services
logger = StructuredLogger("relevance-engine")

# Exemples logging
logger.info("Calculating relevance score", 
            agent_id="agent-123", 
            channel_id="channel-456", 
            factors_count=5)

logger.error("Failed to retrieve agent state", 
             agent_id="agent-123", 
             error="AgentNotFound", 
             stack_trace=traceback.format_exc())
```

### M√©triques Prometheus
```python
# monitoring/metrics.py
from prometheus_client import Counter, Histogram, Gauge, Info
import time
from functools import wraps
from typing import Callable, Any

# M√©triques globales
REQUEST_COUNT = Counter(
    'almaa_requests_total',
    'Total requests by endpoint and method',
    ['method', 'endpoint', 'status']
)

REQUEST_DURATION = Histogram(
    'almaa_request_duration_seconds',
    'Request duration in seconds',
    ['method', 'endpoint']
)

AGENT_STATES = Gauge(
    'almaa_agent_states',
    'Number of agents by state',
    ['state']
)

RELEVANCE_SCORES = Histogram(
    'almaa_relevance_scores',
    'Distribution of relevance scores',
    ['agent_type']
)

OLLAMA_REQUESTS = Counter(
    'almaa_ollama_requests_total',
    'Total requests to Ollama',
    ['model', 'status']
)

OLLAMA_DURATION = Histogram(
    'almaa_ollama_duration_seconds',
    'Ollama request duration',
    ['model']
)

MEMORY_OPERATIONS = Counter(
    'almaa_memory_operations_total',
    'Memory operations',
    ['operation', 'agent_id']
)

# D√©corateurs pour m√©triques automatiques
def track_requests(endpoint: str = None):
    """D√©corateur pour tracker requ√™tes HTTP."""
    def decorator(func: Callable) -> Callable:
        @wraps(func)
        async def wrapper(*args, **kwargs):
            endpoint_name = endpoint or func.__name__
            start_time = time.time()
            
            try:
                result = await func(*args, **kwargs)
                REQUEST_COUNT.labels(
                    method='POST',  # √Ä adapter selon contexte
                    endpoint=endpoint_name,
                    status='success'
                ).inc()
                return result
            except Exception as e:
                REQUEST_COUNT.labels(
                    method='POST',
                    endpoint=endpoint_name,
                    status='error'
                ).inc()
                raise
            finally:
                REQUEST_DURATION.labels(
                    method='POST',
                    endpoint=endpoint_name
                ).observe(time.time() - start_time)
                
        return wrapper
    return decorator

def track_ollama_calls(model: str):
    """D√©corateur pour tracker appels Ollama."""
    def decorator(func: Callable) -> Callable:
        @wraps(func)
        async def wrapper(*args, **kwargs):
            start_time = time.time()
            
            try:
                result = await func(*args, **kwargs)
                OLLAMA_REQUESTS.labels(model=model, status='success').inc()
                return result
            except Exception:
                OLLAMA_REQUESTS.labels(model=model, status='error').inc()
                raise
            finally:
                OLLAMA_DURATION.labels(model=model).observe(time.time() - start_time)
                
        return wrapper
    return decorator

# Exemple usage
@track_requests('calculate_relevance')
async def calculate_relevance_score(agent_id: str, context: Any) -> float:
    # Logique calcul...
    score = 0.75
    RELEVANCE_SCORES.labels(agent_type='developer').observe(score)
    return score
```

---

## ‚ö†Ô∏è S√âCURIT√â ET VALIDATION

### Validation Inputs
```python
# core/validation.py
from pydantic import BaseModel, validator, ValidationError
from typing import Any, Dict, List, Optional
import re
import html

class InputValidator:
    """Validateur inputs s√©curis√©."""
    
    @staticmethod
    def sanitize_string(value: str) -> str:
        """Sanitise cha√Æne de caract√®res."""
        if not isinstance(value, str):
            raise ValueError("Input must be string")
            
        # HTML escaping
        sanitized = html.escape(value)
        
        # Suppression caract√®res dangereux
        sanitized = re.sub(r'[<>"\']', '', sanitized)
        
        # Limitation longueur
        if len(sanitized) > 10000:
            raise ValueError("String too long (max 10000 chars)")
            
        return sanitized.strip()
    
    @staticmethod
    def validate_agent_id(agent_id: str) -> str:
        """Valide format ID agent."""
        if not re.match(r'^[a-zA-Z0-9_-]{1,50}$', agent_id):
            raise ValueError("Invalid agent ID format")
        return agent_id
    
    @staticmethod
    def validate_model_name(model: str) -> str:
        """Valide nom mod√®le Ollama."""
        allowed_models = [
            "llama3.1:8b", "codellama:7b", "mistral:7b", 
            "phi3:3.8b", "gemma:2b", "qwen:4b"
        ]
        if model not in allowed_models:
            raise ValueError(f"Model {model} not allowed")
        return model
    
    @staticmethod
    def validate_file_path(path: str) -> str:
        """Valide chemin fichier s√©curis√©."""
        # Emp√™che path traversal
        if '..' in path or path.startswith('/'):
            raise ValueError("Invalid file path")
            
        # Caract√®res autoris√©s
        if not re.match(r'^[a-zA-Z0-9_/.-]+$', path):
            raise ValueError("Invalid characters in path")
            
        return path

class SecureRequest(BaseModel):
    """Base pour requ√™tes s√©curis√©es."""
    
    class Config:
        validate_assignment = True
        extra = "forbid"  # Emp√™che champs suppl√©mentaires
        
    @validator('*', pre=True)
    def sanitize_strings(cls, v):
        if isinstance(v, str):
            return InputValidator.sanitize_string(v)
        return v
```

### Sandbox Ex√©cution
```python
# security/sandbox.py
import docker
import tempfile
import os
import shutil
from typing import Dict, Any, Optional
import asyncio
from pathlib import Path

class DockerSandbox:
    """Sandbox Docker pour ex√©cution s√©curis√©e outils."""
    
    def __init__(self):
        self.client = docker.from_env()
        self.base_image = "python:3.11-alpine"
        self.network_name = "almaa_sandbox"
        
    async def execute_tool(
        self,
        tool_name: str,
        code: str,
        files: Dict[str, str] = None,
        env_vars: Dict[str, str] = None,
        timeout: int = 300
    ) -> Dict[str, Any]:
        """Ex√©cute outil dans sandbox s√©curis√©."""
        
        # Cr√©ation environnement temporaire
        with tempfile.TemporaryDirectory() as tmp_dir:
            workspace = Path(tmp_dir)
            
            # √âcriture fichiers
            if files:
                for filename, content in files.items():
                    file_path = workspace / filename
                    file_path.parent.mkdir(parents=True, exist_ok=True)
                    file_path.write_text(content)
            
            # Script principal
            main_script = workspace / "main.py"
            main_script.write_text(code)
            
            # Configuration container
            container_config = {
                'image': self.base_image,
                'command': f'python /workspace/main.py',
                'working_dir': '/workspace',
                'volumes': {
                    str(workspace): {
                        'bind': '/workspace',
                        'mode': 'rw'
                    }
                },
                'environment': env_vars or {},
                'network_mode': self.network_name,
                'mem_limit': '512m',
                'cpu_quota': 50000,  # 50% CPU
                'security_opt': ['no-new-privileges'],
                'cap_drop': ['ALL'],
                'read_only': True,
                'tmpfs': {'/tmp': 'noexec,nosuid,size=100m'},
                'detach': True,
                'remove': True
            }
            
            try:
                # Ex√©cution container
                container = self.client.containers.run(**container_config)
                
                # Attente avec timeout
                result = await asyncio.wait_for(
                    self._wait_container_completion(container),
                    timeout=timeout
                )
                
                return result
                
            except asyncio.TimeoutError:
                # Timeout ‚Üí kill container
                try:
                    container.kill()
                except:
                    pass
                return {
                    'success': False,
                    'error': f'Tool execution timeout after {timeout}s',
                    'stdout': '',
                    'stderr': '',
                    'exit_code': -1
                }
            except Exception as e:
                return {
                    'success': False,
                    'error': f'Sandbox execution error: {e}',
                    'stdout': '',
                    'stderr': '',
                    'exit_code': -1
                }
    
    async def _wait_container_completion(self, container) -> Dict[str, Any]:
        """Attend compl√©tion container et r√©cup√®re r√©sultats."""
        
        # Attente fin ex√©cution
        exit_code = container.wait()['StatusCode']
        
        # R√©cup√©ration logs
        logs = container.logs()
        stdout = logs.decode('utf-8', errors='ignore')
        
        # R√©cup√©ration fichiers output si existants
        output_files = {}
        try:
            # Exemple : r√©cup√©ration fichier r√©sultats
            output_tar = container.get_archive('/workspace/output')[0]
            # Processing tar si n√©cessaire...
        except:
            pass
        
        return {
            'success': exit_code == 0,
            'exit_code': exit_code,
            'stdout': stdout,
            'stderr': '',  # Docker combine stdout/stderr
            'output_files': output_files,
            'execution_time': 0  # √Ä calculer si n√©cessaire
        }
    
    def setup_network(self):
        """Configure r√©seau isol√© pour sandboxes."""
        try:
            # R√©seau sans acc√®s internet
            self.client.networks.create(
                name=self.network_name,
                driver="bridge",
                internal=True,  # Pas d'acc√®s externe
                options={
                    "com.docker.network.bridge.enable_icc": "false"  # Isolation inter-containers
                }
            )
        except docker.errors.APIError as e:
            if "already exists" not in str(e):
                raise
```

---

## üöÄ D√âPLOIEMENT ET TESTS

### Scripts de D√©veloppement
```bash
#!/bin/bash
# scripts/dev-setup.sh

set -e

echo "üöÄ Setting up ALMAA Workspace development environment..."

# V√©rification Docker
if ! command -v docker &> /dev/null; then
    echo "‚ùå Docker required but not installed"
    exit 1
fi

if ! command -v docker-compose &> /dev/null; then
    echo "‚ùå Docker Compose required but not installed"
    exit 1
fi

# Cr√©ation r√©seaux Docker
echo "üì° Creating Docker networks..."
docker network create almaa-network 2>/dev/null || echo "Network already exists"
docker network create almaa-sandbox --internal 2>/dev/null || echo "Sandbox network already exists"

# Variables environnement
if [ ! -f .env ]; then
    echo "‚öôÔ∏è Creating environment configuration..."
    cat > .env << EOF
# Database
DB_PASSWORD=$(openssl rand -base64 32)
POSTGRES_USER=almaa
POSTGRES_DB=almaa

# MinIO
MINIO_ACCESS_KEY=almaa-admin
MINIO_SECRET_KEY=$(openssl rand -base64 32)

# Monitoring
GRAFANA_PASSWORD=$(openssl rand -base64 16)

# Security
JWT_SECRET_KEY=$(openssl rand -base64 64)
ENCRYPTION_KEY=$(openssl rand -base64 32)

# Development
DEBUG=true
LOG_LEVEL=DEBUG
ENVIRONMENT=development
EOF
    echo "‚úÖ Environment configuration created (.env)"
fi

# G√©n√©ration certificats SSL auto-sign√©s
if [ ! -d "nginx/ssl" ]; then
    echo "üîê Generating SSL certificates..."
    mkdir -p nginx/ssl
    openssl req -x509 -nodes -days 365 -newkey rsa:2048 \
        -keyout nginx/ssl/almaa.key \
        -out nginx/ssl/almaa.crt \
        -subj "/C=FR/ST=Local/L=Local/O=ALMAA/CN=almaa.local"
    echo "‚úÖ SSL certificates generated"
fi

# Build images d√©veloppement
echo "üèóÔ∏è Building development images..."
docker-compose -f docker-compose.dev.yml build

# Initialisation base de donn√©es
echo "üóÉÔ∏è Initializing database..."
docker-compose -f docker-compose.dev.yml up -d postgres redis
sleep 10

# Migrations
docker-compose -f docker-compose.dev.yml exec postgres psql -U almaa -d almaa -c "
CREATE EXTENSION IF NOT EXISTS \"uuid-ossp\";
CREATE EXTENSION IF NOT EXISTS \"pgvector\";
"

# Test connexions
echo "üîç Testing connections..."
docker-compose -f docker-compose.dev.yml up -d ollama chromadb minio
sleep 15

# Validation Ollama
echo "Testing Ollama..."
curl -sf http://localhost:11434/api/tags > /dev/null && echo "‚úÖ Ollama OK" || echo "‚ùå Ollama failed"

# Validation ChromaDB
echo "Testing ChromaDB..."
curl -sf http://localhost:8000/api/v1/heartbeat > /dev/null && echo "‚úÖ ChromaDB OK" || echo "‚ùå ChromaDB failed"

# Validation MinIO
echo "Testing MinIO..."
curl -sf http://localhost:9001 > /dev/null && echo "‚úÖ MinIO OK" || echo "‚ùå MinIO failed"

# D√©marrage complet
echo "üöÄ Starting complete development environment..."
docker-compose -f docker-compose.dev.yml up -d

echo ""
echo "üéâ Development environment ready!"
echo "üìä Admin Dashboard: https://localhost (admin/admin)"
echo "üìä Grafana: http://localhost:3000 (admin/$(grep GRAFANA_PASSWORD .env | cut -d'=' -f2))"
echo "üóÉÔ∏è MinIO Console: http://localhost:9001 (almaa-admin/SECRET)"
echo "üîß API Docs: http://localhost:8000/docs"
echo ""
echo "üìù Next steps:"
echo "1. Load Ollama models: docker-compose exec ollama ollama pull llama3.1:8b"
echo "2. Run tests: ./scripts/run-tests.sh"
echo "3. View logs: docker-compose logs -f"
```

### Tests Automatis√©s
```bash
#!/bin/bash
# scripts/run-tests.sh

set -e

echo "üß™ Running ALMAA Workspace test suite..."

# Tests unitaires Python
echo "üìã Running Python unit tests..."
docker-compose -f docker-compose.test.yml run --rm api pytest tests/unit/ -v --cov=src --cov-report=term-missing

# Tests int√©gration
echo "üìã Running integration tests..."
docker-compose -f docker-compose.test.yml run --rm api pytest tests/integration/ -v

# Tests frontend
echo "üìã Running frontend tests..."
docker-compose -f docker-compose.test.yml run --rm frontend npm test

# Tests end-to-end
echo "üìã Running E2E tests..."
docker-compose -f docker-compose.test.yml run --rm e2e-tests

# V√©rifications s√©curit√©
echo "üîí Running security checks..."
docker-compose -f docker-compose.test.yml run --rm security-scanner

# Tests performance
echo "‚ö° Running performance tests..."
docker-compose -f docker-compose.test.yml run --rm load-tests

# Nettoyage
echo "üßπ Cleaning up test environment..."
docker-compose -f docker-compose.test.yml down -v

echo "‚úÖ All tests passed!"
```

---

## üìñ CONCLUSION POUR GPT-5

Ce guide complet te donne tous les √©l√©ments n√©cessaires pour d√©velopper ALMAA Workspace V2.0 avec succ√®s :

### üéØ **Priorit√©s Absolues**
1. **Syst√®me de Pertinence** ‚Üí Agents parlent quand c'est pertinent
2. **Interface Admin** ‚Üí Contr√¥le complet des agents
3. **Templates Agents** ‚Üí Sp√©cialisation et configuration
4. **Communication Inter-Agents** ‚Üí MPs et canaux deepthink

### üõ†Ô∏è **Standards Qualit√©**
- Code document√© et test√©
- Validation s√©curis√©e de tous inputs
- Logging structur√© pour debugging
- M√©triques Prometheus int√©gr√©es
- Configuration flexible YAML

### üöÄ **Approche Recommand√©e**
1. **Analyse Code Existant** ‚Üí Comprendre l'architecture actuelle
2. **D√©veloppement Incr√©mental** ‚Üí Extensions pas reconstruction
3. **Tests Continus** ‚Üí Validation √† chaque √©tape
4. **Documentation** ‚Üí Explique chaque choix technique

### üí¨ **Communication**
- Pose des questions sur l'architecture existante
- Explique tes choix techniques
- Signale les compromis ou limitations
- Propose des am√©liorations d'architecture

**Ensemble, nous allons cr√©er l'assistant AGI personnel le plus avanc√© et s√©curis√© possible ! üöÄ**